{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import patsy\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.core.display import display, HTML\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import diagnostic_plots\n",
    "from numpy import linalg as LA\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip Scraping by loading Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Chromedriver\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Empty Lists for Features\n",
    "resort = []\n",
    "snowfall = []\n",
    "acres = []\n",
    "vertical = []\n",
    "summit = []\n",
    "lifts = []\n",
    "rating = []\n",
    "state = []\n",
    "links = []\n",
    "zranklinks = ['https://zrankings.com/?_=1531866037152&amp;page=1', 'https://zrankings.com/?_=1531866037152&amp;page=2','https://zrankings.com/?_=1531866037152&amp;page=3','https://zrankings.com/?_=1531866037152&amp;page=4','https://zrankings.com/?_=1531866037152&amp;page=5','https://zrankings.com/?_=1531866037152&amp;page=6','https://zrankings.com/?_=1531866037152&amp;page=7','https://zrankings.com/?_=1531866037152&amp;page=8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through pages on Zrankings.com to collect resort information\n",
    "for x in zranklinks:\n",
    "    driver.get(x)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    links = links + [link['href'] for link in soup.find_all('a',class_='btn-more-snow-index more-profile')]\n",
    "    table = soup.find('table', class_='index-table-2017')\n",
    "    resort = resort + [title.text.strip().replace('\\n','').replace('\\t','').replace('Resort','').replace('Ski','').replace('Area','')[:-3] for title in table.find_all(class_='name-rank')]\n",
    "    state = state + [title.text.strip().replace('\\n','').replace('\\t','')[-3:] for title in table.find_all(class_='name-rank')]\n",
    "    snowfall = snowfall + [int(snow.text[0:3]) for snow in table.find_all(class_='desktop-375')[2::2]]\n",
    "    acres = acres + [int(area.text.replace(',','')[:-6]) for area in table.find_all(class_='desktop-750')[2::2]]\n",
    "    vertical = vertical + [int(vert.text.replace(',','')[:-3]) for vert in table.find_all(class_='desktop-620')[2::2]]\n",
    "    summit = summit + [int(summ.text.replace(',','')[:-3]) for summ in table.find_all(class_='desktop-850')[2::2]]\n",
    "    lifts = lifts + [int(lift.text) for lift in table.find_all(class_='desktop-950')[2::2]]\n",
    "    rating = rating + [float(score.text) for score in table.find_all(class_='index-score-cell right-border-chrome')]\n",
    "    time.sleep(0.5)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty lists for the loop\n",
    "runs = []\n",
    "longest = []\n",
    "uphillmax = []\n",
    "parks = []\n",
    "snowboard = []\n",
    "nams = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through individual resort pages to scrape additional features\n",
    "for resortinfo in links:\n",
    "    try:\n",
    "        req = requests.get('https://www.zrankings.com{}'.format(resortinfo))\n",
    "        soup = BeautifulSoup(req.text, 'html5lib')\n",
    "        sup = soup.find(class_='desktop-name').find('h3')\n",
    "        nams.append(sup.text.strip().replace('Ski','').replace('Resort','').replace('Area',''))\n",
    "        resortstats = soup.find('div', class_='side-stats-2 clearfix')\n",
    "        runs = runs + [int(run) for run in resortstats.find_all('span')[1]]\n",
    "        longest = longest + [int(long[:-3].replace(',','')) for long in resortstats.find_all('span')[2]]\n",
    "        uphillmax = uphillmax + [uphill.replace(',','')[:-6] for uphill in resortstats.find_all('span')[4]]\n",
    "        parks = parks + [terrain for terrain in resortstats.find_all('span')[5]]\n",
    "        snowboard = snowboard + [board=='Yes' for board in resortstats.find_all('span')[6]]\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame\n",
    "df = pd.DataFrame({'resort_name': resort,\n",
    "                   'state' : state,\n",
    "                  'annual_snowfall': snowfall,\n",
    "                  'acres': acres,\n",
    "                  'vertical': vertical,\n",
    "                  'summit': summit,\n",
    "                  'lifts': lifts,\n",
    "                  'paf_score': rating})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe of secondary features\n",
    "df2 = pd.DataFrame({'resort': nams,\n",
    "                   'runs' : runs,\n",
    "                   'longest_run' : longest,\n",
    "                   'uphill_capacity' : uphillmax,\n",
    "                   'park_terrain': parks,\n",
    "                   'snowboard' : snowboard})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all features into one dataframe\n",
    "df3 = pd.merge(left=df,right=df2, how='left', left_on='resort_name', right_on='resort')\n",
    "df3.uphill_capacity = df3.uphill_capacity.str.strip()\n",
    "df3[['park_terrain','uphill_capacity']] = df3[['park_terrain','uphill_capacity']].apply(pd.to_numeric)\n",
    "df3 = df3.drop(['resort'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape Ticket Prices\n",
    "response = requests.get('https://www.onthesnow.com/united-states/lift-tickets.html')\n",
    "soup = BeautifulSoup(response.text, 'html5lib')\n",
    "td = soup.find_all('div',class_='name')\n",
    "skires = [res.text.replace('Ski','').replace('Resort','').replace('Area','') for res in soup.find_all('div',class_='name')]\n",
    "tr = soup.find_all('tr', class_=\"rowB\")\n",
    "tabledata = soup.find_all('td',class_='rLeft')\n",
    "ticket = [cost.text[4:] for cost in tabledata[2::5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Ticket Prices DataFrame\n",
    "ticketprices = pd.DataFrame({'ski_resort': skires,\n",
    "                            'TicketPrice': ticket})\n",
    "ticketprices['TicketPrice'].replace('', np.nan, inplace=True)\n",
    "ticketprices.dropna(subset=['TicketPrice'], inplace=True)\n",
    "ticketprices[['TicketPrice']] = ticketprices[['TicketPrice']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge ticket prices with resort features\n",
    "merged_inner = pd.merge(left=df3,right=ticketprices, how='left', left_on='resort_name', right_on='ski_resort')\n",
    "merged_inner['ski_resort'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in blank ticket prices\n",
    "csv = pd.read_csv('fuzz_list.csv')\n",
    "merged_inner.TicketPrice=merged_inner.TicketPrice.fillna(csv['Ticket Price'])\n",
    "csv = pd.read_csv('ticketprice.csv')\n",
    "merged_inner.TicketPrice=merged_inner.TicketPrice.fillna(csv['TicketPrice'])\n",
    "new_df = merged_inner\n",
    "new_df = new_df.drop(['ski_resort'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset.TicketPrice=trainingset.TicketPrice.fillna(csv['TicketPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset.drop(['resort_name', 'state'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowboardset = pd.get_dummies(trainingset['snowboard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = trainingset.merge(snowboardset, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pickle from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = trainingset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainingset = pd.get_dummies((trainingset['snowboard']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"modifiedtrainingset.pkl\", 'rb') as picklefile: \n",
    "    trainingset = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and y\n",
    "X = trainingset.drop(['TicketPrice', 'snowboard'],1)\n",
    "X = X.fillna(X.mean())\n",
    "y = pd.DataFrame(trainingset['TicketPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Features\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaled_df = scaler.fit_transform(X)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns = ['annual_snowfall','acres','vertical','summit','lifts','paf_score','runs','longest_run','uphill_capacity','park_terrain','False','True'])\n",
    "df_train = pd.concat([scaled_df,y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "X = df_train.iloc[:, :-1]\n",
    "y = df_train.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(y.replace(0, y.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_train.corr(), cmap=\"seismic\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X.copy().drop(['acres','summit','lifts','paf_score','longest_run','True','False'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X2 = patsy.dmatrices('TicketPrice ~ vertical + runs + uphill_capacity + park_terrain + annual_snowfall', data = df_train2, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = sm.OLS(y, X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression() \n",
    "lr2.fit(X_train, y_train)\n",
    "lr2.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit2 = model2.fit()\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train3 = df_train2.copy().drop(['False'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train3['runs_2'] = df_train3['runs']**(1/2)\n",
    "df_train3['vertical_2'] = df_train3['vertical']**2\n",
    "df_train3['uphill_capacity_2'] = df_train3['uphill_capacity']**3\n",
    "df_train3['annual_snowfall_2'] = df_train3['annual_snowfall']**2\n",
    "X = df_train3.drop(['TicketPrice'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n",
    "lr3 = LinearRegression() \n",
    "fit3 = lr3.fit(X_train, y_train)\n",
    "y2_predict = lr3.predict(X_train)\n",
    "RMSE = np.sqrt(mean_squared_error(np.exp(y2_predict), np.exp(y_train)))\n",
    "print(RMSE)\n",
    "lr3.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_pred3 = lr3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted vs Actual Plot\n",
    "plt.scatter(test_set_pred3,y_test,alpha=0.2);\n",
    "plt.plot(np.linspace(2,6,2),np.linspace(2,6,2));\n",
    "plt.title('Predicted vs Actual')\n",
    "ax.set_ylabel('Actual', fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diagnostic Plots\n",
    "diagnostic_plots.diagnostic_plots(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso GridSearch CV\n",
    "lasso = Lasso()\n",
    "params = {\n",
    "    'alpha': [-100, -5, -0.1, -0.5, 0, 0.001, 0.01, 0.1, 98, 100],\n",
    "    'max_iter': [500, 1000, 2000, 4000]\n",
    "}\n",
    "grid = GridSearchCV(lasso, param_grid=params, cv=3, scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_, grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.resid.plot(style='o', figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model values\n",
    "model_fitted_y = fit.fittedvalues\n",
    "# model residuals\n",
    "model_residuals = fit.resid\n",
    "# normalized residuals\n",
    "model_norm_residuals = fit.get_influence().resid_studentized_internal\n",
    "# absolute squared normalized residuals\n",
    "model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n",
    "# absolute residuals\n",
    "model_abs_resid = np.abs(model_residuals)\n",
    "# leverage, from statsmodels internals\n",
    "model_leverage = fit.get_influence().hat_matrix_diag\n",
    "# cook's distance, from statsmodels internals\n",
    "model_cooks = fit.get_influence().cooks_distance[0]\n",
    "\n",
    "plot_lm_1 = plt.figure()\n",
    "plot_lm_1.axes[0] = sns.residplot(model_fitted_y, trainingset.columns[-1], data=trainingset,\n",
    "                          lowess=True,\n",
    "                          scatter_kws={'alpha': 0.5},\n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "plot_lm_1.axes[0].set_title('Residuals vs Fitted')\n",
    "plot_lm_1.axes[0].set_xlabel('Fitted values')\n",
    "plot_lm_1.axes[0].set_ylabel('Residuals');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
